{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell Support (tf.nn.rnn_cell)\n",
    "- BasicRNNCell: The most basic RNN cell\n",
    "- RNNCell: Abstract object representing an RNN cell\n",
    "- BasicLSTMCell: Basic LSTM recurrent network cell\n",
    "- LSTMCell: LSTM recurrent network cell\n",
    "- GRUCell: Gated Recurrent Unit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A clean, no_frills character-level generative language model.\n",
    "CS 20: \"TensorFlow for Deep Learning Research\"\n",
    "cs20.stanford.edu\n",
    "Danijar Hafner (mail@danijar.com)\n",
    "& Chip Huyen (chiphuyen@cs.stanford.edu)\n",
    "Lecture 11\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window, overlap):\n",
    "    lines = [line.strip() for line in open(filename, 'r').readlines()]\n",
    "    while True:\n",
    "        random.shuffle(lines)\n",
    "\n",
    "        for text in lines:\n",
    "            text = vocab_encode(text, vocab)\n",
    "            for start in range(0, len(text) - window, overlap):\n",
    "                chunk = text[start: start + window]\n",
    "                chunk += [0] * (window - len(chunk))\n",
    "                yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.path = 'data/' + model + '.txt'\n",
    "        if 'trump' in model:\n",
    "            self.vocab = (\"$%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \" '\\\"_abcdefghijklmnopqrstuvwxyz{|}@#âž¡ðŸ“ˆ\")\n",
    "        else:\n",
    "            self.vocab = (\" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "\n",
    "        self.seq = tf.placeholder(tf.int32, [None, None])\n",
    "        self.temp = tf.constant(1.5)\n",
    "        self.\n",
    "        _sizes = [128, 256]\n",
    "        self.batch_size = 64\n",
    "        self.lr = 0.0003\n",
    "        self.skip_step = 1\n",
    "        self.num_steps = 50 # for RNN unrolled\n",
    "        self.len_generated = 200\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def create_rnn(self, seq):\n",
    "        layers = [tf.nn.rnn_cell.GRUCell(size) for size in self.hidden_sizes]\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        batch = tf.shape(seq)[0]\n",
    "        zero_states = cells.zero_state(batch, dtype=tf.float32)\n",
    "        self.in_state = tuple([tf.placeholder_with_default(state, [None, state.shape[1]]) \n",
    "                                for state in zero_states])\n",
    "        # this line to calculate the real length of seq\n",
    "        # all seq are padded to be of the same length, which is num_steps\n",
    "        length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "        self.output, self.out_state = tf.nn.dynamic_rnn(cells, seq, length, self.in_state)\n",
    "\n",
    "    def create_model(self):\n",
    "        seq = tf.one_hot(self.seq, len(self.vocab))\n",
    "        self.create_rnn(seq)\n",
    "        self.logits = tf.layers.dense(self.output, len(self.vocab), None)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits[:, :-1], \n",
    "                                                        labels=seq[:, 1:])\n",
    "        self.loss = tf.reduce_sum(loss)\n",
    "        # sample the next character from Maxwell-Boltzmann Distribution \n",
    "        # with temperature temp. It works equally well without tf.exp\n",
    "        self.sample = tf.multinomial(tf.exp(self.logits[:, -1] / self.temp), 1)[:, 0] \n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.gstep)\n",
    "\n",
    "    def train(self):\n",
    "        saver = tf.train.Saver()\n",
    "        start = time.time()\n",
    "        min_loss = None\n",
    "        with tf.Session() as sess:\n",
    "            writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/' + self.model + '/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            iteration = self.gstep.eval()\n",
    "            stream = read_data(self.path, self.vocab, self.num_steps, overlap=self.num_steps//2)\n",
    "            data = read_batch(stream, self.batch_size)\n",
    "            while True:\n",
    "                batch = next(data)\n",
    "\n",
    "            # for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "                batch_loss, _ = sess.run([self.loss, self.opt], {self.seq: batch})\n",
    "                if (iteration + 1) % self.skip_step == 0:\n",
    "                    print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                    self.online_infer(sess)\n",
    "                    start = time.time()\n",
    "                    checkpoint_name = 'checkpoints/' + self.model + '/char-rnn'\n",
    "                    if min_loss is None:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                    elif batch_loss < min_loss:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                        min_loss = batch_loss\n",
    "                iteration += 1\n",
    "\n",
    "    def online_infer(self, sess):\n",
    "        \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "        \"\"\"\n",
    "        for seed in ['Hillary', 'I', 'R', 'T', '@', 'N', 'M', '.', 'G', 'A', 'W']:\n",
    "            sentence = seed\n",
    "            state = None\n",
    "            for _ in range(self.len_generated):\n",
    "                batch = [vocab_encode(sentence[-1], self.vocab)]\n",
    "                feed = {self.seq: batch}\n",
    "                if state is not None: # for the first decoder step, the state is None\n",
    "                    for i in range(len(state)):\n",
    "                        feed.update({self.in_state[i]: state[i]})\n",
    "                index, state = sess.run([self.sample, self.out_state], feed)\n",
    "                sentence += vocab_decode(index, self.vocab)\n",
    "            print('\\t' + sentence)\n",
    "\n",
    "def main():\n",
    "    model = 'trump_tweets'\n",
    "    utils.safe_mkdir('checkpoints')\n",
    "    utils.safe_mkdir('checkpoints/' + model)\n",
    "\n",
    "    lm = CharRNN(model)\n",
    "    lm.create_model()\n",
    "    lm.train()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct cells\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "\n",
    "# stack multiple cells\n",
    "layers = [tf.nn.rnn_cell.GRUCell(size) for size in hidden_sizes]\n",
    "cells = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)\n",
    "# Most sequences are not of the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.nn.dynamic_rnn: uses a tf.While loop to dynamically construct the graph when it is executed. Graph creation is faster and you can feed batches of variable size.\n",
    "- tf.nn.bidirectional_dynamic_rnn: dynamic_rnn with bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padded/truncated sequence length\n",
    "### Approach 1:\n",
    "- Maintain a mask (True for real, False for padded tokens)\n",
    "- Run your model on both the real/padded tokens (model will predict labels for the padded tokens as well)\n",
    "- Only take into account the loss caused by the real elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_loss = tf.nn.softmax_cross_entropy_with_logits(preds, labels)\n",
    "loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2:\n",
    "- Let your model know the real sequence length so it only predict the labels for the real tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients\n",
    "### Use different activation units:\n",
    "- tf.nn.relu\n",
    "- tf.nn.relu6\n",
    "- tf.nn.crelu\n",
    "- tf.nn.elu\n",
    "\n",
    "### In addition to:\n",
    "- tf.nn.softplus\n",
    "- tf.nn.softsign\n",
    "- tf.nn.bias_add\n",
    "- tf.sigmoid\n",
    "- tf.tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding Gradients\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
